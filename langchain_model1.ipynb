{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LangChain Model with Groq Llama3\n",
                "\n",
                "This Jupyter Notebook demonstrates how to implement a LangChain model using the Groq Llama3 model. The notebook guides you through the steps of setting up the environment, defining tools, creating prompts, and running an agent to fetch information."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Set up the Environment\n",
                "\n",
                "Before running the model, ensure that you have your API keys from Groq and Tavily set as environment variables."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "language": "python"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "from langchain_groq import ChatGroq\n",
                "\n",
                "# --- Step 3 is here: Put your new Groq API Key in this line ---\n",
                "os.environ['GROQ_API_KEY'] = \"PUT_YOUR_GROQ_API_KEY_HERE\"\n",
                "\n",
                "# Safety check\n",
                "if os.environ.get(\"GROQ_API_KEY\") == \"PUT_YOUR_GROQ_API_KEY_HERE\":\n",
                "    raise ValueError(\"Please replace the placeholder with your real Groq API key.\")\n",
                "\n",
                "# Initialize the model (using Llama 3, which is very fast on Groq)\n",
                "llm = ChatGroq(\n",
                "    model=\"llama3-8b-8192\",\n",
                "    temperature=0.7\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: The LLM (The \"Brain\")\n",
                "\n",
                "We'll use the fast Groq Llama3 model as the agent's reasoning engine."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: The Tools (The \"Toolbox\")\n",
                "\n",
                "An agent needs tools to interact with the world. For this example, our agent's only tool is the Tavily search engine."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "language": "python"
            },
            "outputs": [],
            "source": [
                "from langchain_community.tools.tavily_search import TavilySearchResults\n",
                "\n",
                "# Define the tools\n",
                "tools = [TavilySearchResults(max_results=3)]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: The Prompt\n",
                "\n",
                "The prompt is a special set of instructions that tells the LLM how to behave as an agent. We pull a pre-built, high-quality prompt from the LangChain Hub to save time."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "language": "python"
            },
            "outputs": [],
            "source": [
                "from langchain import hub\n",
                "\n",
                "# Pull the prompt from LangChain Hub\n",
                "prompt = hub.pull(\"hwchase17/react\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Create the Agent\n",
                "\n",
                "This function combines the LLM, the tools, and the prompt into a runnable agent."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "language": "python"
            },
            "outputs": [],
            "source": [
                "from langchain.agents import AgentExecutor, create_react_agent\n",
                "\n",
                "# Create the agent\n",
                "agent = create_react_agent(llm, tools, prompt)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: The Agent Executor\n",
                "\n",
                "The AgentExecutor is the runtime that makes the agent work. It invokes the agent, calls the chosen tools, and passes the results back to the agent. Setting `verbose=True` is essential for understanding how the agent is \"thinking\"."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "language": "python"
            },
            "outputs": [],
            "source": [
                "# Create the agent executor\n",
                "agent_executor = AgentExecutor(\n",
                "    agent=agent,\n",
                "    tools=tools,\n",
                "    verbose=True,\n",
                "    handle_parsing_errors=True  # <-- ADD THIS LINE\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Run the Agent\n",
                "\n",
                "Let's ask a question that requires up-to-date information."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "language": "python"
            },
            "outputs": [],
            "source": [
                "# Run the agent\n",
                "response = agent_executor.invoke({\n",
                "    \"input\": \"What were some major headlines in India this week, in August 2025?\"\n",
                "})\n",
                "\n",
                "print(\"\\n--- FINAL ANSWER ---\")\n",
                "print(response['output'])"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
